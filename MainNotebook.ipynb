{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Load Data","metadata":{"_uuid":"ca6861e4-0e8f-4d3e-a670-d2f23a401320","_cell_guid":"0e15e399-a7b3-4686-9a3d-5922d6749916","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-15T22:21:58.904908Z","iopub.execute_input":"2023-03-15T22:21:58.905271Z","iopub.status.idle":"2023-03-15T22:22:13.130331Z","shell.execute_reply.started":"2023-03-15T22:21:58.905240Z","shell.execute_reply":"2023-03-15T22:22:13.129142Z"}}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport json\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\n\ntrain_loc = '../input/herbarium-2022-fgvc9/train_images/'\n\nwith open(\"../input/herbarium-2022-fgvc9/train_metadata.json\") as json_file:\n    train_loc = json.load(json_file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# JSON to Dataframe","metadata":{}},{"cell_type":"code","source":"image_ids = [image[\"image_id\"] for image in train_meta[\"images\"]]\nimage_dirs = [train_dir + image['file_name'] for image in train_meta[\"images\"]]\ncategory_ids = [annotation['category_id'] for annotation in train_meta['annotations']]\ngenus_ids = [annotation['genus_id'] for annotation in train_meta['annotations']]\n\ntest_ids = [image['image_id'] for image in test_meta]\ntest_dirs = [test_dir + image['file_name'] for image in test_meta]\n\n#Create the initial training dataframe with the above defined columns\nmain_df = pd.DataFrame({\n    \"id\" : image_ids,\n    \"directory\" : image_dirs,\n    \"category\" : category_ids,\n    \"genus\" : genus_ids})\n\nmain_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Mapping genus and family","metadata":{}},{"cell_type":"code","source":"#Add a genus column to the dataframe\ngenus_map = {genus['genus_id'] : genus['genus'] for genus in train_meta['genera']}\nmain_df['genus'] = main_df['genus'].map(genus_map)\n\n##Create a family column in the datagframe based on the genus names\n    # Step 1: Create dictionary of genus -> family mapping\ngenus_family_map = {}\nfor category in train_meta[\"category\"]:\n    genus = category['genus']\n    family = category['family']\n    genus_family_map[genus] = family\n\n    # Step 2: Create new column with default value of Noneâ„¢\nmain_df['family'] = None\n\n    # Step 3: Update values in new column based on genus -> family mapping\nfor i, row in main_df.iterrows():\n    genus = row['genus']\n    if genus in genus_family_map:\n        family = genus_family_map[genus]\n        main_df.at[i, 'family'] = family\n\nmain_df","metadata":{"_uuid":"f3df79ec-dcce-4644-871c-83042ff211f2","_cell_guid":"e50f78f2-95de-47f7-a054-84df03f95d16","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Filtering to Poaceae","metadata":{}},{"cell_type":"code","source":"#Filter only the images of plants that are in the Poaceae family\nmain_df = main_df.loc[main_df['family'] == 'Poaceae']\n#Reset index\nmain_df = main_df.reset_index(drop=True)\n\nmain_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Adding species column","metadata":{}},{"cell_type":"code","source":"#Add a species column if deemed necessary\n\"\"\"\nmain_df[\"species\"] = None\n\n# Extract category_id and species values from categories where the family is Poaceae\nspecies_list = []\nfor category in train_meta[\"categories\"]:\n    if category[\"family\"] == \"Poaceae\":\n        species_list.append({\n            \"category\": category[\"category_id\"],\n            \"species\": category[\"species\"]\n        })\n\n# loop through data frame and species list to update species column\nfor i, row in main_df.iterrows():\n    for species in species_list:\n        if row['category'] == species['category']:\n            main_df.at[i, 'species'] = species['species']\"\"\"","metadata":{"_uuid":"5a7f2cb1-59e0-4b29-94c8-c073b88a9b35","_cell_guid":"989f3b10-1540-4184-9035-9f1f27b094a0","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA ðŸ–¼ ","metadata":{}},{"cell_type":"code","source":"genus_data = main_df['genus'].value_counts().head(15)\ngenus_data = pd.DataFrame({'Genus' : genus_data.index,\n                     'values' : genus_data.values})\n                     \nplt.figure(figsize = (20, 10))\nsns.barplot(x='values', y = 'Genus', data = genus_data , palette='summer_r')\nplt.show()\n\n#From most to least: Muhlenbergia, Paspalum, Poa, Dichanthelium, Sporobolus, Eragrostis etc.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image displaying","metadata":{}},{"cell_type":"code","source":"def show_images(genus):\n    images = main_df.loc[main_df['genus'] == genus]['image_dir'][:9]\n    i = 1\n    fig = plt.figure(figsize = (18, 18))\n    plt.suptitle(genus, fontsize = '30')\n    for image in images:\n        img = cv2.imread(image)\n        ax = fig.add_subplot(3, 3, i)\n        ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n        ax.set_axis_off()\n        i += 1\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_images(\"Muhlenbergia\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main_df[\"genus\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference âš¡âš¡âš¡","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom torchvision.models import resnet50, ResNet50_Weights\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom PIL import Image\n\n!pip install fvcore","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main_df['genus'] = pd.factorize(main_df['genus'])[0]\nmain_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Separate data into 3 sets","metadata":{}},{"cell_type":"code","source":"grouped = main_df.groupby('genus')\n# Split the dataset for each class separately\ntrain_dfs = []\ntest_dfs = []\n\nfor name, group in grouped:\n    # Split the group into train and test sets\n    train, test = train_test_split(group, test_size=0.2, random_state=42)\n    # Add the train and test dataframes to the respective lists\n    train_dfs.append(train)\n    test_dfs.append(test)\n\n# Concatenate the training and evaluation sets for all classes into single DataFrames\ntrain_df_initial = pd.concat(train_dfs)\ntrain_df_initial = train_df_initial.reset_index(drop=True)\n\ntest_df = pd.concat(test_dfs)\ntest_df = test_df.reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grouped2 = train_df_initial.groupby('genus')\n# Split the dataset for each class separately\ntrain_dfs = []\nval_dfs = []\n\nfor name, group in grouped2:\n    # Split the group into train and test sets\n    train, validation = train_test_split(group, test_size=0.2, random_state=42)\n    # Add the train and test dataframes to the respective lists\n    train_dfs.append(train)\n    val_dfs.append(validation)\n\n# Concatenate the training and evaluation sets for all classes into single DataFrames\ntrain_df = pd.concat(train_dfs)\ntrain_df = train_df.reset_index(drop=True)\n\nval_df = pd.concat(val_dfs)\nval_df = val_df.reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\nepochs = 3\nIM_SIZE = 224\nlearning_rate = 1e-4\n\nX_train, Y_train = train_df[\"image_dir\"].values, train_df[\"genus\"].values\n\nX_val, Y_val = val_df[\"image_dir\"].values, val_df[\"genus\"].values\n\nX_test, Y_test  = test_df[\"image_dir\"].values, test_df[\"genus\"].values\n\nTransform = transforms.Compose([\n     transforms.Resize((IM_SIZE, IM_SIZE)),\n     transforms.ToTensor(),\n     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GetData(Dataset):\n    def __init__(self, FNames, Labels, Transform):\n        self.fnames = FNames\n        self.transform = Transform\n        self.labels = Labels         \n        \n    def __len__(self):\n        return len(self.fnames)\n\n    def __getitem__(self, index):       \n        x = Image.open(self.fnames[index])\n        return self.transform(x), self.labels[index]\n        \n        \ntrainset = GetData(X_train, Y_train, Transform)\ntrainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n\nvalset = GetData(X_val, Y_val, Transform)\nvalloader = DataLoader(valset, batch_size=batch_size, shuffle=True)\n\ntestset = GetData(X_test, Y_test, Transform)\ntestloader = DataLoader(testset, batch_size=batch_size, shuffle=True)\n\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nmodel = torch.hub.load('zhanghang1989/ResNeSt', 'resnest101', pretrained=True)            ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_classes = train_df['genus'].nunique()\ntotal_layers = len(list(model.parameters()))\n\nfor param in model.parameters():\n    param.requires_grad = True\n    \nn_inputs = model.fc.in_features\nlast_layer = nn.Linear(n_inputs, num_classes)\nmodel.fc = last_layer \n\nif torch.cuda.is_available():\n    model.cuda()\n    \nprint(model.fc.out_features)    \n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training and evaluation","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nfrom sklearn.metrics import f1_score\n\ndef train(trainloader, model, criterion, optimizer, scaler, device=torch.device(\"cpu\")):\n    model.train()\n    train_acc = 0.0\n    train_loss = 0.0\n    y_true = []\n    y_pred = []\n    for images, labels in tqdm(trainloader):\n        images = images.to(device)\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        with torch.cuda.amp.autocast(enabled=True):\n            output = model(images)\n            loss = criterion(output, labels)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            acc = ((output.argmax(dim=1) == labels).float().mean())\n            train_acc += acc\n            train_loss += loss\n            y_true += labels.cpu().numpy().tolist()\n            y_pred += output.argmax(dim=1).cpu().numpy().tolist()\n            \n    train_f1 = f1_score(y_true, y_pred, average=None)\n    train_f1_avg = f1_score(y_true, y_pred, average='macro')\n    return train_acc/len(trainloader), train_loss/len(trainloader), train_f1, train_f1_avg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(testloader, model, criterion, device=torch.device(\"cpu\")):\n    model.eval()\n    eval_acc = 0.0\n    eval_loss = 0.0\n    y_true = []\n    y_pred = []\n    for images, labels in tqdm(testloader):\n        images = images.to(device)\n        labels = labels.to(device)\n        with torch.no_grad():\n            output = model(images)\n            loss = criterion(output, labels)\n        acc = ((output.argmax(dim=1) == labels).float().mean())\n        eval_acc += acc\n        eval_loss += loss\n        y_true += labels.cpu().numpy().tolist()\n        y_pred += output.argmax(dim=1).cpu().numpy().tolist()\n  \n    eval_f1 = f1_score(y_true, y_pred, average=None)\n    eval_f1_avg = f1_score(y_true, y_pred, average='macro')\n    return eval_acc/len(testloader), eval_loss/len(testloader), eval_f1, eval_f1_avg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = torch.cuda.amp.GradScaler(enabled=True)\n\ntrain_f1_scores = []  # Initialize an empty list to store training F1 scores\nval_f1_scores = []  # Initialize an empty list to store validation F1 scores\n\n\nfor epoch in range(epochs):\n    train_acc, train_loss, train_f1, train_f1_avg = train(trainloader, model, criterion, optimizer, scaler, device=device)\n    eval_acc, eval_loss, eval_f1, eval_f1_avg = evaluate(valloader, model, criterion, device=torch.device(\"cuda\"))\n\n    train_f1_scores.append(train_f1_avg)  # Store the training F1 score for this epoch\n    val_f1_scores.append(eval_f1_avg)  # Store the validation F1 score for this epoch\n\n    print(f\"Epoch {epoch + 1} | Train Acc: {train_acc*100} | Train Loss: {train_loss} | Train F1 (Avg): {train_f1_avg}\")\n    print(f\"\\t Val Acc: {eval_acc*100} | Val Loss: {eval_loss} | Val F1 (Avg): {eval_f1_avg}\")\n    \n    print(\"F1 score per class (Train):\")\n    for i, f1 in enumerate(train_f1):\n        print(f\"Class {i}: {f1}\")\n\n    print(\"\\nF1 score per class (Validation):\") \n    for i, f1 in enumerate(eval_f1):\n        print(f\"Class {i}: {f1}\")\n\n    print(\"====\"*8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot the training results ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nplt.plot(train_f1_scores, label=\"Train F1 (Avg)\")\nplt.plot(val_f1_scores, label=\"Validation F1 (Avg)\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"F1 Score\")\nplt.title(f\"F1 Scores Over Epochs\")\nplt.legend()\nplt.xticks(range(len(train_f1_scores)), range(1, len(train_f1_scores) + 1))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing loop","metadata":{}},{"cell_type":"code","source":"# Assume that the `testloader` DataLoader has been created for the test set\ntest_acc = 0.0\ny_true = []\ny_pred = []\n\nmodel = model.to(device)\n\n# Set the model to evaluation mode\nmodel.eval()\n\n# Loop through the test set\nwith torch.no_grad():\n    for images, labels in tqdm(testloader, desc=\"Testing\"):\n        images = images.to(device)\n        labels = labels.to(device)\n\n        # Make predictions\n        output = model(images)\n\n        # Calculate accuracy\n        acc = (output.argmax(dim=1) == labels).float().mean()\n        test_acc += acc\n\n        # Store true labels and predicted labels for other metrics\n        y_true += labels.cpu().numpy().tolist()\n        y_pred += output.argmax(dim=1).cpu().numpy().tolist()\n\n# Calculate the final test accuracy\ntest_acc = test_acc / len(testloader)\nprint(f\"Test accuracy: {test_acc:.2%}\")\n\n# Calculate the classification report, which includes the F1 score for all classes\nreport = classification_report(y_true, y_pred, output_dict=True)\n\n# Print the classification report\nfor class_label, metrics in report.items():\n    if class_label.isdigit():\n        print(f\"Class {class_label}: F1 score = {metrics['f1-score']:.2f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}